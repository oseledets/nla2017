{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Lecture-13:--Iterative-methods-for-large-scale-eigenvalue-problems\" data-toc-modified-id=\"Lecture-13:--Iterative-methods-for-large-scale-eigenvalue-problems-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lecture 13:  Iterative methods for large scale eigenvalue problems</a></div><div class=\"lev2 toc-item\"><a href=\"#Previous-lecture\" data-toc-modified-id=\"Previous-lecture-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Previous lecture</a></div><div class=\"lev2 toc-item\"><a href=\"#Partial-eigenvalue-problem\" data-toc-modified-id=\"Partial-eigenvalue-problem-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Partial eigenvalue problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Power-method-and-related-methods\" data-toc-modified-id=\"Power-method-and-related-methods-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Power method and related methods</a></div><div class=\"lev3 toc-item\"><a href=\"#Power-method\" data-toc-modified-id=\"Power-method-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Power method</a></div><div class=\"lev3 toc-item\"><a href=\"#Inverse-iteration\" data-toc-modified-id=\"Inverse-iteration-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Inverse iteration</a></div><div class=\"lev3 toc-item\"><a href=\"#Rayleigh-quotient-(RQ)-iteration\" data-toc-modified-id=\"Rayleigh-quotient-(RQ)-iteration-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Rayleigh quotient (RQ) iteration</a></div><div class=\"lev3 toc-item\"><a href=\"#Inexact-inverse-iteration-framework\" data-toc-modified-id=\"Inexact-inverse-iteration-framework-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Inexact inverse iteration framework</a></div><div class=\"lev3 toc-item\"><a href=\"#Block-power-method\" data-toc-modified-id=\"Block-power-method-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Block power method</a></div><div class=\"lev3 toc-item\"><a href=\"#Accelerating-convergence-of-the-block-power-method\" data-toc-modified-id=\"Accelerating-convergence-of-the-block-power-method-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Accelerating convergence of the block power method</a></div><div class=\"lev2 toc-item\"><a href=\"#Ritz-approximation\" data-toc-modified-id=\"Ritz-approximation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Ritz approximation</a></div><div class=\"lev3 toc-item\"><a href=\"#Properties-of-the-Ritz-approximation\" data-toc-modified-id=\"Properties-of-the-Ritz-approximation-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Properties of the Ritz approximation</a></div><div class=\"lev3 toc-item\"><a href=\"#Rayleigh-Ritz-method\" data-toc-modified-id=\"Rayleigh-Ritz-method-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span><font color=\"red\">Rayleigh-Ritz method</font></a></div><div class=\"lev2 toc-item\"><a href=\"#Lanczos-and-Arnoldi-methods\" data-toc-modified-id=\"Lanczos-and-Arnoldi-methods-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Lanczos and Arnoldi methods</a></div><div class=\"lev3 toc-item\"><a href=\"#Why-is-$\\theta_\\max-\\approx-\\lambda_\\max$?\" data-toc-modified-id=\"Why-is-$\\theta_\\max-\\approx-\\lambda_\\max$?-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Why is <span class=\"MathJax_Preview\">\\theta_\\max \\approx \\lambda_\\max</span><script type=\"math/tex\">\\theta_\\max \\approx \\lambda_\\max</script>?</a></div><div class=\"lev3 toc-item\"><a href=\"#Demo:-approximating-largest-eigenvalue-with-Lanczos\" data-toc-modified-id=\"Demo:-approximating-largest-eigenvalue-with-Lanczos-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Demo: approximating largest eigenvalue with Lanczos</a></div><div class=\"lev3 toc-item\"><a href=\"#Practical-issues-and-stability\" data-toc-modified-id=\"Practical-issues-and-stability-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Practical issues and stability</a></div><div class=\"lev3 toc-item\"><a href=\"#More-problems-with-the-Lanczos-method\" data-toc-modified-id=\"More-problems-with-the-Lanczos-method-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>More problems with the Lanczos method</a></div><div class=\"lev2 toc-item\"><a href=\"#PINVIT-(preconditioned-inverse-iteration)\" data-toc-modified-id=\"PINVIT-(preconditioned-inverse-iteration)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>PINVIT (preconditioned inverse iteration)</a></div><div class=\"lev3 toc-item\"><a href=\"#Derivation\" data-toc-modified-id=\"Derivation-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Derivation</a></div><div class=\"lev3 toc-item\"><a href=\"#Convergence-theory\" data-toc-modified-id=\"Convergence-theory-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Convergence theory</a></div><div class=\"lev3 toc-item\"><a href=\"#Block-case\" data-toc-modified-id=\"Block-case-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>Block case</a></div><div class=\"lev2 toc-item\"><a href=\"#LOBPCG-(Locally-Optimal-Block-Preconditioned-CG)\" data-toc-modified-id=\"LOBPCG-(Locally-Optimal-Block-Preconditioned-CG)-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>LOBPCG (Locally Optimal Block Preconditioned CG)</a></div><div class=\"lev3 toc-item\"><a href=\"#Locally-optimal-PCG-(not-&quot;Block&quot;-so-far-:))\" data-toc-modified-id=\"Locally-optimal-PCG-(not-&quot;Block&quot;-so-far-:))-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Locally optimal PCG (not \"Block\" so far :))</a></div><div class=\"lev3 toc-item\"><a href=\"#LOPCG-(stable-version)\" data-toc-modified-id=\"LOPCG-(stable-version)-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>LOPCG (stable version)</a></div><div class=\"lev3 toc-item\"><a href=\"#Locally-optimal--block--PCG\" data-toc-modified-id=\"Locally-optimal--block--PCG-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Locally optimal <font color=\"red\"> block </font> PCG</a></div><div class=\"lev3 toc-item\"><a href=\"#LOBPCG-summary\" data-toc-modified-id=\"LOBPCG-summary-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>LOBPCG summary</a></div><div class=\"lev2 toc-item\"><a href=\"#Jacobi-Davidson-(JD)-method\" data-toc-modified-id=\"Jacobi-Davidson-(JD)-method-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Jacobi-Davidson (JD) method</a></div><div class=\"lev3 toc-item\"><a href=\"#JD-derivation\" data-toc-modified-id=\"JD-derivation-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>JD derivation</a></div><div class=\"lev3 toc-item\"><a href=\"#Jacobi-correction-equation\" data-toc-modified-id=\"Jacobi-correction-equation-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Jacobi correction equation</a></div><div class=\"lev3 toc-item\"><a href=\"#Solving-Jacobi-correction-equation\" data-toc-modified-id=\"Solving-Jacobi-correction-equation-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Solving Jacobi correction equation</a></div><div class=\"lev3 toc-item\"><a href=\"#Connection-to-the-Rayleigh-quotient-iteration\" data-toc-modified-id=\"Connection-to-the-Rayleigh-quotient-iteration-1.8.4\"><span class=\"toc-item-num\">1.8.4&nbsp;&nbsp;</span>Connection to the Rayleigh quotient iteration</a></div><div class=\"lev3 toc-item\"><a href=\"#Preconditioning-of-Jacobi-equation\" data-toc-modified-id=\"Preconditioning-of-Jacobi-equation-1.8.5\"><span class=\"toc-item-num\">1.8.5&nbsp;&nbsp;</span>Preconditioning of Jacobi equation</a></div><div class=\"lev3 toc-item\"><a href=\"#Subspace-acceleration-in-JD\" data-toc-modified-id=\"Subspace-acceleration-in-JD-1.8.6\"><span class=\"toc-item-num\">1.8.6&nbsp;&nbsp;</span>Subspace acceleration in JD</a></div><div class=\"lev3 toc-item\"><a href=\"#The-block-case-of-JD\" data-toc-modified-id=\"The-block-case-of-JD-1.8.7\"><span class=\"toc-item-num\">1.8.7&nbsp;&nbsp;</span>The block case of JD</a></div><div class=\"lev3 toc-item\"><a href=\"#Jacobi-Davidson:-summary\" data-toc-modified-id=\"Jacobi-Davidson:-summary-1.8.8\"><span class=\"toc-item-num\">1.8.8&nbsp;&nbsp;</span>Jacobi-Davidson: summary</a></div><div class=\"lev2 toc-item\"><a href=\"#Software\" data-toc-modified-id=\"Software-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Software</a></div><div class=\"lev2 toc-item\"><a href=\"#Take-home-message\" data-toc-modified-id=\"Take-home-message-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Take-home message</a></div><div class=\"lev2 toc-item\"><a href=\"#Next-lecture\" data-toc-modified-id=\"Next-lecture-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Next lecture</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 13:  Iterative methods for large scale eigenvalue problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture\n",
    "\n",
    "- Finalizing iterative methods for linear systems (minres, bicg, bicgstab)\n",
    "\n",
    "- Jacobi, Gauss-Seidel, SSOR methods as preconditioners\n",
    "\n",
    "- Incomplete LU for preconditioning, three flavours: ILU(k), ILUT, ILU2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partial eigenvalue problem\n",
    "\n",
    "Recall that to find eigenvalues of matrix of size $N\\times N$ one can use, e.g. the QR algorithm.\n",
    "\n",
    "However, in some applications matrix is so large, that we even can not store it exactly.\n",
    "\n",
    "Typically such matrices are given as a **black-box** that is able only to multiply matrix by vector (sometimes even without access to matrix elements). This is what we assume today.\n",
    "\n",
    "In this case the best we can do is to solve partial eigenvalue problem, e.g.\n",
    "\n",
    "- Find $k\\ll N$ smallest or largest eigenvalues (and eigenvectors if needed)\n",
    "- Find $k\\ll N$ eigenvalues closest to a given number $\\sigma$\n",
    "\n",
    "For simplicity we will consider case when matrix is normal and thus has orthonormal basis of eigenvectors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power method and related methods\n",
    "\n",
    "### Power method\n",
    "\n",
    "Recall that the simplest method to find the largest eigenvalue is the **power method**\n",
    "$$\n",
    "    x_{i+1} = \\frac{Ax_{i}}{\\|Ax_{i}\\|}\n",
    "$$\n",
    "The convergence is linear with rate $q = \\left|\\frac{\\lambda_1}{\\lambda_2}\\right|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse iteration\n",
    "\n",
    "To find the smallest eigenvalue one may run power method for $A^{-1}$:\n",
    "\n",
    "$$x_{i+1} = \\frac{A^{-1}x_{i}}{\\|A^{-1}x_{i}\\|}.$$\n",
    "\n",
    "To accelerate convergence <font color='red'>shift-and-invert</font> strategy can be used:\n",
    "\n",
    "$$x_{i+1} = \\frac{(A-\\sigma I)^{-1}x_{i}}{\\|(A-\\sigma I)^{-1}x_{i}\\|},$$\n",
    "\n",
    "where $\\sigma$ should be close to the eigenvalue we want to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rayleigh quotient (RQ) iteration\n",
    "\n",
    "In order to get superlinear convergence one may use adaptive shifts:\n",
    "\n",
    "$$x_{i+1} = \\frac{(A-R(x_i) I)^{-1}x_{i}}{\\|(A-R(x_i) I)^{-1}x_{i}\\|},$$\n",
    "\n",
    "where $R(x_k) = \\frac{(x_i, Ax_i)}{(x_i, x_i)}$ is Rayleigh quotient. \n",
    "\n",
    "The method converges **cubically for Hermitian matrices** and quadratically for non-Hermitian case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inexact inverse iteration framework\n",
    "\n",
    "Matrices $(A- \\sigma I)$ as well as $(A-R(x_i) I)$ are ill-conditioned if $\\sigma$ or $R(x_i)$ are close to eigenvalues.\n",
    "\n",
    "Thus, if you are not given e.g. LU factorization of such matrix you might face a problem.\n",
    "\n",
    "In practice you can solve systems only with some accuracy. Recall also that condition number is an upper bound and is overestimated for cosistent rhs. So, even in RQ iteration  letting\n",
    "the shift tend to the eigenvalue [does not harm](http://www.sciencedirect.com/science/article/pii/S0024379505005756) significantly\n",
    "the performance of the iterative methods.\n",
    "\n",
    "If accuracy of solution of systems increases from iteration to iteration, superlinear convergence for RQ iteration can still be present [[Theorem 2.1](http://www.sciencedirect.com/science/article/pii/S0024379505005756)].\n",
    "Otherwise, you will get linear convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Block power method\n",
    "\n",
    "The block power method (also known as subspace iteration method or simultaneous vector iteration) is a natural generalization of the power method for several largest eigenvalues.<br>\n",
    "It looks as:\n",
    "\n",
    "1. $Y_0$ is $N\\times k$ matrix of rank $k$, $Y_0 = X_0 R_0$ (QR-decomposition)\n",
    "2. $Y_i = AX_{i-1}$ \n",
    "3. $Y_i = X_i R_i$ (QR-decomposition)\n",
    "\n",
    "QR-decomposition plays role of normalization in the standard power method. \n",
    "\n",
    "Moreover, orthogonalization prevents the columns of the $X_i$ from converging all to the eigenvector corresponding to the largest modulus eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accelerating convergence of the block power method\n",
    "\n",
    "* For Hermitian matrices convergence of the $j$-column is **linear** as for the power method with $q=\\frac{|\\lambda_{j}|}{|\\lambda_{j+1}|}$. \n",
    "\n",
    "\n",
    "* Hence, applying the block power method to the matrix $(A-\\sigma I)^{-1}$ will accelerate convergence (<font color='red'>shift-and-invert</font> strategy).\n",
    "\n",
    "\n",
    "* You can also accelerate the convergence by applying the **Rayleigh-Ritz procedure** discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x112f19ed0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAESCAYAAADqoDJEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119cd1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "n = 100\n",
    "k = 10\n",
    "A = np.diag(1./(1. + np.arange(n))) # diagonal matrix with well-separated maximum eigenvalues\n",
    "A_clustered = np.diag(1 - 1./(1. + np.arange(n))) # diagonal matrix with clustered maximum eigenvalues\n",
    "\n",
    "def subspace_iter(A, Y0, num_iter=100):\n",
    "    Y0, _ = np.linalg.qr(Y0)\n",
    "    Y = Y0.copy()\n",
    "    Y_old = Y0.copy()\n",
    "    err = []\n",
    "    for i in range(num_iter):\n",
    "        X = A.dot(Y)\n",
    "        Y, _ = np.linalg.qr(X)\n",
    "        err.append(np.linalg.norm(Y_old - Y.dot(Y.T.dot(Y_old))))\n",
    "        Y_old = Y.copy()\n",
    "    return Y, err\n",
    "\n",
    "Y0 = np.random.random((n, k))\n",
    "Y, err = subspace_iter(A, Y0, num_iter=100)\n",
    "Y, err_clustered = subspace_iter(A_clustered, Y0, num_iter=100) #np.diag((diagonal - sigma)**(-1))\n",
    "plt.semilogy(err, label='Separated eigvals')\n",
    "plt.semilogy(err_clustered, label='Clustered eigvals')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we go to advanced methods let us discuss the important concept of **Ritz approximation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ritz approximation\n",
    "\n",
    "Given subspace spanned by columns of unitary matrix $Q_k$ of size $N\\times k$ we consider the projected matrix $Q_k^* A Q_k$.\n",
    "\n",
    "Let $\\Theta_k=\\mathrm{diag}(\\theta_1,\\dots,\\theta_k)$ and $S_k=\\begin{bmatrix}s_1 & \\dots & s_k \\end{bmatrix}$ be matrices of eigenvalues and eigenvectors of $(Q_k^* A Q_k)$: \n",
    "\n",
    "$$(Q_k^* A Q_k)S_k = S_k \\Theta_k$$\n",
    "\n",
    "then $\\{\\theta_i\\}$ are called **Ritz values** and $y_i = Q_k s_i$ - **Ritz vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the Ritz approximation\n",
    "\n",
    "- Note that they are not eigenpairs of the initial matrix $AY_k\\not= Y_k \\Theta_k$, but the following equality holds:\n",
    "\n",
    "    $$Q_k^* (AY_k - Y_k \\Theta_k) = Q_k^* (AQ_k S_k - Q_k S_k \\Theta_k) = 0,$$\n",
    "\n",
    "   so the residual for the Ritz approximation is **orthogonal** to the subspace spanned by columns of $Q_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- $\\lambda_\\min(A) \\leq \\theta_\\min \\leq \\theta_\\max \\leq \\lambda_\\max(A)$. Indeed, using Rayleigh quotient:\n",
    "\n",
    "    $$\\theta_\\min = \\lambda_\\min (Q_k^* A Q_k) = \\min_{x\\not=0} \\frac{x^* (Q_k^* A Q_k) x}{x^* x} = \\min_{y\\not=0:y=Q_k x} \\frac{y^*  A y}{y^* y}\\geq \\min_{y\\not= 0} \\frac{y^*  A y}{y^* y} = \\lambda_\\min(A).$$\n",
    "\n",
    "    Obviously, $\\lambda_\\min (Q_k^* A Q_k) = \\lambda_\\min(A)$ if $k=N$, but we want to construct a basis $k\\ll N$ such that $\\lambda_\\min (Q_k^* A Q_k) \\approx \\lambda_\\min(A)$.\n",
    "\n",
    "    Similarly, $\\theta_\\max \\leq \\lambda_\\max(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='red'>Rayleigh-Ritz method</font>\n",
    "\n",
    "Thus, if a subspace $V$ approximates first $k$ eigenvectors, then one can use the **Rayleigh-Ritz method**:\n",
    "\n",
    "1. Find orthonormal basis $Q_k$ in $V$ (e.g. by using QR decomposition)\n",
    "2. Calculate $Q_k^*AQ_k$\n",
    "3. Compute Ritz values and vectors\n",
    "4. Note that alternatevly one could use $V$ with no orthogonalization, but then generalized eigenvalue problem $(V^*AV)s_i = \\theta_i (V^*V)s_i$ has to be solved.\n",
    "\n",
    "The question is how to find a good subspace $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lanczos and Arnoldi methods\n",
    "\n",
    "The good choice for $V$ is the Krylov subspace.\n",
    "\n",
    "Recall that in the power method we used only one Krylov vector $$x_k = \\frac{A^k x_0}{\\|A^k x_0\\|}.$$\n",
    "\n",
    "In this case $\\theta_k = \\frac{x_k^* A x_k}{x_k^* x_k}$ is nothing but a Ritz value. Natural idea is to use a bigger Krylov subspace.\n",
    "\n",
    "As a result we can find more eigenvalues (power method only gives $\\lambda_\\max$). Moreover,convergence of the eigenvalue corresponding to $\\lambda_\\max$ will be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Hermitian matrices from the Arnoldi relation we have\n",
    "\n",
    "$$\n",
    "Q_k^*AQ_k = T_k,\n",
    "$$\n",
    "\n",
    "where $Q_k$ is orthogonal basis in the Krylov subspace generated by the Lanczos procedure and $T_k$ is triangular matrix.\n",
    "\n",
    "According to the Rayleigh-Ritz method we expect that eigenvalues of $T_k$ approximate eigenvalues of $A$. This method is called the **Lanczos method**. For nonsymmetric matrices it is called the **Arnoldi method** and instead of tridiagonal $T_k$ we would get upper=Hessenberg matrix.\n",
    "\n",
    "Let us show that  $\\theta_\\max \\approx\\lambda_\\max$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why is $\\theta_\\max \\approx \\lambda_\\max$?\n",
    "\n",
    "Let us denote $\\theta_1 \\equiv \\theta_\\max$ and $\\lambda_1 \\equiv \\lambda_\\max$. Then\n",
    "\n",
    "$$\n",
    "    \\theta_1 = \\max_{y\\in \\mathcal{K}_i, y\\not=0}\\frac{(y,Ay)}{(y,y)} = \\max_{p_{i-1}} \\frac{(p_{i-1}(A)x_0, A p_{i-1}(A)x_0)}{(p_{i-1}(A)x_0, p_{i-1}(A)x_0)},\n",
    "$$\n",
    "\n",
    "where $p_{i-1}$ is a polynomial of degree not greater than $i-1$ such that $p_{i-1}(A)x_0\\not=0$.\n",
    "\n",
    "Expand $x_0 = \\sum_{j=1}^N c_j v_j$, where $v_j$ are eigenvectors of $A$ (form orthonormal basis).\n",
    "\n",
    "Since $\\theta_1 \\leq \\lambda_1$ we get\n",
    "$$\n",
    "    \\lambda_1 - \\theta_1 \\leq \\lambda_1 - \\frac{(p_{i-1}(A)x_0, A p_{i-1}(A)x_0)}{(p_{i-1}(A)x_0, p_{i-1}(A)x_0)}\n",
    "$$\n",
    "for any polynomial $p_{i-1}$. Hence\n",
    "$$\n",
    "\\lambda_1 - \\theta_1 \\leq \\lambda_1 - \\frac{\\sum_{k=1}^N \\lambda_k |p_{i-1}(\\lambda_k)|^2 |c_k|^2}{\\sum_{k=1}^N |p_{i-1}(\\lambda_k)|^2 |c_k|^2} =\n",
    "$$\n",
    "$$\n",
    "= \\frac{\\sum_{k=2}^N (\\lambda_1 - \\lambda_k) |p_{i-1}(\\lambda_k)|^2 |c_k|^2}{|p_{i-1}(\\lambda_1)|^2 |c_1|^2 + \\sum_{k=2}^N |p_{i-1}(\\lambda_k)|^2 |c_k|^2} \\leq \n",
    "(\\lambda_1 - \\lambda_n) \\frac{\\max_{2\\leq k \\leq N}|p_{i-1}(\\lambda_k)|^2}{|p_{i-1}(\\lambda_1)|^2 }\\gamma, \\quad \\gamma = \\frac{\\sum_{k=2}^N|c_k|^2}{|c_1|^2}\n",
    "$$\n",
    "\n",
    "Since the inequality holds for any polynomial $p_{i-1}$ we will choose a polynomial: \n",
    "\n",
    "$$|p_{i-1}(\\lambda_1)| \\gg \\max_{2\\leq k \\leq N}|p_{i-1}(\\lambda_k)|.$$\n",
    "\n",
    "This holds, e.g. for the Chebyshev polynomial on $[\\lambda_n,\\lambda_2]$. Thus, $\\theta_1 \\approx \\lambda_1$ or more precisely (Paige-Kaniel error bound, check it!):\n",
    "$$\n",
    "    \\lambda_1 - \\theta_1 \\leq \\frac{\\lambda_1 - \\lambda_n}{T_{i-1}^2(1 + 2\\mu)}\\gamma, \\quad \\mu = \\frac{\\lambda_1 - \\lambda_2}{\\lambda_2 - \\lambda_n},\n",
    "$$\n",
    "where $T_{i-1}$ is a Chebyshev polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo: approximating largest eigenvalue with Lanczos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=10, err = 0.107582626192\n",
      "k=20, err = 0.0687683516265\n",
      "k=100, err = 1.10218323357e-09\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import scipy.sparse.linalg\n",
    "import copy\n",
    "n = 40\n",
    "ex = np.ones(n)\n",
    "lp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr')\n",
    "e = sp.sparse.eye(n)\n",
    "A = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\n",
    "\n",
    "def lanczos(A, m): #Arpack is ...!! Does not allow any callback. Even does not return number of iterations!!!\n",
    "    n = A.shape[0]\n",
    "    v = np.random.random((n, 1))\n",
    "    v = v / np.linalg.norm(v)\n",
    "    v_old = np.zeros((n, 1))\n",
    "    beta = np.zeros(m)\n",
    "    alpha = np.zeros(m)\n",
    "    for j in range(m-1):\n",
    "        w = A.dot(v)\n",
    "        alpha[j] = w.T.dot(v)\n",
    "        w = w - alpha[j] * v - beta[j] * v_old\n",
    "        beta[j+1] = np.linalg.norm(w)\n",
    "        v_old = v.copy()\n",
    "        v = w / beta[j+1]\n",
    "    w = A.dot(v)\n",
    "    alpha[m-1] = w.T.dot(v)\n",
    "    A = np.diag(beta[1:], k=-1) + np.diag(beta[1:], k=1) + np.diag(alpha[:], k=0)\n",
    "    l, _ = np.linalg.eigh(A)\n",
    "    return l\n",
    "\n",
    "# Approximation of the largest eigenvalue for different k\n",
    "l_large_exact = sp.sparse.linalg.eigsh(A, k=99, which='LM')[0][0]\n",
    "print('k=10, err = {}'.format(np.abs(l_large_exact - lanczos(A, 10)[0])))\n",
    "print('k=20, err = {}'.format(np.abs(l_large_exact - lanczos(A, 20)[0])))\n",
    "print('k=100, err = {}'.format(np.abs(l_large_exact - lanczos(A, 100)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical issues and stability\n",
    "The Lanczos vectors may loose orthogonality during the process due to floating-point errors, thus all practical implementations of it use **restarts**.\n",
    "\n",
    "A very good introduction to the topic is given in the book of **Golub and Van-Loan (Matrix Computations)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More problems with the Lanczos method\n",
    "\n",
    "- Applying Lanczos directly to $A$ may result into a very slow convergence if $\\lambda_i\\approx \\lambda_{i+1}$ <br> (typically holds for smallest eigenvalues that are not well-separated)\n",
    "\n",
    "\n",
    "- To accelerate the convergence one may apply Lanczos to $(A-\\sigma I)^{-1}$, but in this case systems have to be solved **very accurately**. <br>\n",
    "Otherwise the Arnoldi relation does not hold anymore.\n",
    "\n",
    "An alternative to this approach are the so-called preconditioned iterative methods that include:\n",
    "1. PINVIT (Preconditioned Inverse Iteration)\n",
    "2. LOBCPG (Locally optimal block preconditioned CG)\n",
    "3. Jacobi-Davidson method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PINVIT (preconditioned inverse iteration)\n",
    "\n",
    "### Derivation\n",
    "\n",
    "Consider Rayleigh quotient $R(x) = \\frac{(x,Ax)}{(x,x)}$. Then\n",
    "$$\n",
    "\\nabla R(x) = \\frac{2}{(x,x)} (Ax - R(x) x),\n",
    "$$\n",
    "\n",
    "so the simplest gradient descent method with a preconditioner $B$ reads\n",
    "\n",
    "$$\n",
    "    x_{i+1} = x_{i} - \\tau_i B^{-1} (Ax_i - R(x_i) x_i),\n",
    "$$\n",
    "\n",
    "$$\n",
    "    x_{i+1} = \\frac{x_{i+1}}{\\|x_{i+1}\\|}.\n",
    "$$\n",
    "\n",
    "Typically $B\\approx (A-\\sigma I)$, where $\\sigma$ is called shift.\n",
    "\n",
    "The closer $\\sigma$ to the required eigenvalue is, the faster the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parameter $\\tau_k$ is chosen to minimize the $R(x_{i+1})$ over $\\tau_k$ (steepest descent method).\n",
    "\n",
    "One can think of this minimization procedure as minimization in basis $V = [x_i, r_i]$, where $r_{i}=B^{-1} (Ax_i - R(x_i) x_i)$.\n",
    "\n",
    "This results into the generalized eigenvalue problem $(V^*AV)\\begin{bmatrix}1 \\\\ -\\tau_i \\end{bmatrix} = \\theta (V^*V) \\begin{bmatrix}1 \\\\ -\\tau_i \\end{bmatrix}$ (Rayleigh-Ritz procedure with no orthogonalization of $V$). Here $\\theta$ is the closest to the required eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence theory\n",
    "\n",
    "**Theorem** ([Knyazev and Neymeyr](http://www.sciencedirect.com/science/article/pii/S002437950100461X)) \n",
    "\n",
    "Let \n",
    "- $R(x_{i})\\in [\\lambda_j,\\lambda_{j+1}]$\n",
    "- $R(x_{i+1})\\in [R(x_{i}),\\lambda_{j+1}]$ (case $R(x_{i+1})\\in [\\lambda_{j}, R(x_{i})]$ is similar)\n",
    "- $\\|I - B^{-1} A\\|_A \\leq \\gamma < 1$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\left|\\frac{R(x_{i+1}) - \\lambda_j}{R(x_{i+1}) - \\lambda_{j+1}}\\right| < \\left[ 1 - (1-\\gamma)\\left(1 - \\frac{\\lambda_j}{\\lambda_{j+1}}\\right) \\right]^2 \\cdot \\left|\\frac{R(x_{i}) - \\lambda_j}{R(x_{i}) - \\lambda_{j+1}}\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Block case\n",
    "\n",
    "To find, e.g. $k$ eigenvalues one can do a one step of PINVIT for each vector:\n",
    "\n",
    "\n",
    "$$\n",
    "    x^{(j)}_{i+1} = x^{(j)}_{i} - \\tau^{(j)}_i B^{-1} (Ax^{(j)}_i - R(x^{(j)}_i) x^{(j)}_i), \\quad j=1,\\dots,k\n",
    "$$\n",
    "\n",
    "$$\n",
    "    x^{(j)}_{i+1} = \\frac{x^{(j)}_{i+1}}{\\|x^{(j)}_{i+1}\\|}.\n",
    "$$\n",
    "\n",
    "And then orthogonalize them using the QR-decomposition. However, it is better to use the Rayleigh-Ritz procedure:\n",
    "\n",
    "- Set $X^{i}_k = [x^{(1)}_{i},\\dots, x^{(k)}_{i}]$ and $R^{i}_k = [B^{-1}r^{(1)}_{i},\\dots, B^{-1}r^{(k)}_{i}]$, where $r^{(j)}_{i} = Ax^{(j)}_i - R(x^{(j)}_i) x^{(j)}_i$\n",
    "\n",
    "\n",
    "- Set $V = [X^{i}_k, R^{i}_k]$, use Rayleigh-Ritz procedure for $V$ to find new $X^{i+1}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LOBPCG (Locally Optimal Block Preconditioned CG)\n",
    "\n",
    "### Locally optimal PCG (not \"Block\" so far :))\n",
    "LOPCG method\n",
    "$$\n",
    "    x_{i+1} = x_{i} - \\alpha_i B^{-1} (Ax_i - R(x_i) x_i) + \\beta_i x_{i-1} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    x_{i+1} = \\frac{x_{i+1}}{\\|x_{i+1}\\|}.\n",
    "$$\n",
    "\n",
    "\n",
    "is a superior to PINVIT method as it adds to basis not only $x_i$ and $r_i$, but also $x_{i-1}$.\n",
    "\n",
    "However, this interpretation leads to an unstable algorithm as $x_{i}$ is becoming colinear to $x_{i-1}$ as the procedure converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LOPCG (stable version)\n",
    "\n",
    "Knyazev suggested an equivalent stable version, which introduces new vectors $p_i$ (conjugate gradients)\n",
    "\n",
    "$$\n",
    "p_{i+1} = r_{i} + \\beta_i p_{i},\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_{i} + \\alpha_i p_{i+1}.\n",
    "$$\n",
    "\n",
    "One can check that $\\mathcal{L}(x_{i},x_{i-1},r_{i})=\\mathcal{L}(x_{i},p_{i},r_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The stable version explains name of the method:\n",
    "\n",
    "In standard CG method we would minimze Rayleigh quotient $R$ in the conjugate gradient direction $p_{i+1}$: \n",
    "\n",
    "$$\\alpha_i = \\arg\\min_{\\alpha_i} R(x_i + \\alpha_i p_{i+1}).$$\n",
    "\n",
    "In the locally-optimal CG we minimize over two parameters:  \n",
    "\n",
    "$$\\alpha_i, \\beta_i = \\arg\\min_{\\alpha_i,\\beta_i} R\\left(x_i + \\alpha_i p_{i+1}\\right) = \\arg\\min_{\\alpha_i,\\beta_i} R\\left(x_i + \\alpha_i (r_{i} + \\beta_i p_{i})\\right)$$\n",
    "\n",
    "and we locally obtain more optimal solution. That is why the method is called **locally optimal**.\n",
    "\n",
    "As for PINVIT coefficients $\\alpha_i,\\beta_i$ can be found by the Rayleigh-Ritz procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Locally optimal <font color='red'> block </font> PCG\n",
    "\n",
    "In the block version similarly to PINVIT on each iteration we are given basis $V=[X^{(i)}_k,B^{-1}R^{(i)}_k, P^{(i)}_k]$ and use Rayleigh-Ritz procedure.\n",
    "\n",
    "The overall algorithm:\n",
    "\n",
    "1. Find $\\tilde A = V^* A V$\n",
    "2. Find $\\tilde M = V^*V$\n",
    "3. Solve generalized eigenvalue problem $\\tilde A S_k = \\tilde M S_k \\Theta_k$\n",
    "4. $P^{(i+1)}_{k} = [B^{-1}R^{(i)}_k, P^{(i)}_k]S_k[:,k:]$\n",
    "5. $X^{(i+1)}_{k} = X^{(i)}_k S_k[:,:k] + P^{(i+1)}_{k}$ (equivalent to $X^{(i+1)}_{k} = VS_k$)\n",
    "6. Calculate new $B^{-1}R^{(i+1)}_k$\n",
    "7. Set $V=[X^{(i+1)}_k,B^{-1}R^{(i+1)}_k, P^{(i+1)}_k]$, goto 1.\n",
    "\n",
    "**Deflation technique** which stops iterating converged eigestates can also be applied here.\n",
    "\n",
    "The method also converges linearly, but faster than PINVIT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LOBPCG summary\n",
    "\n",
    "- Locally optimal preconditioned solver\n",
    "\n",
    "- Linear convergence\n",
    "\n",
    "- Preconditioner $(A-\\sigma I)$ is not always good for eigenvalue problems\n",
    "\n",
    "The next method (Jacobi-Davidson) has smart preconditioning and superlinear convergence (if systems are solved accurately)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jacobi-Davidson (JD) method\n",
    "\n",
    "Jacobi-Davidson method is a very popular technique for solving eigvalue problems (not only symmetric!).\n",
    "\n",
    "It consits of two **key ingredients**:\n",
    "\n",
    "- Given a preconditioner for $A-R(x_j) I$ it automatically constructs a good preconditioner for the eigevalue problem:\n",
    "$$\n",
    "    B = (I - x_j x^*_j) (A - R(x_j) I) (I - x_j x^*_j),\n",
    "$$\n",
    "where $x_j$ - is approximation to the eigenvector on the $j$-th iteration.<br> **Note** that sometimes approximation to $(A-R(x_j) I)^{-1}$ is not a good preconditioner.\n",
    "\n",
    "\n",
    "- It additionally adds to a subspace $V$ solutions from previous iterations (**subspace acceleration**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### JD derivation\n",
    "\n",
    "Jacobi-Davidson method has a nice manifold optimization interpretation. <br> It is a **Riemannian Newton** method on a sphere and $P = I - x_j x^*_j$ is a projection on a tanget space of a sphere at $x_j$.\n",
    "\n",
    "But we will derive it similarly to the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jacobi correction equation\n",
    "\n",
    "Jacobi not only presents the way to solve the eigenvalue problem by Jacobi rotations, but also proposed an iterative procedure. Let $x_j$ be the current approximation, and $t$ the correction:\n",
    "\n",
    "$$A(x_j + t) = \\lambda (x_j + t),$$\n",
    "\n",
    "and we look for the correction $t \\perp x_j$ (new orthogonal vector).\n",
    "\n",
    "Then, the parallel part has the form\n",
    "\n",
    "$$x_j x^*_j A (x_j + t) = \\lambda x_j,$$\n",
    "\n",
    "which simplifies to \n",
    "\n",
    "$$R(x_j) + x^* _j A t = \\lambda.$$\n",
    "\n",
    "The orthogonal component is \n",
    "\n",
    "$$( I - x_j x^*_j) A (x_j + t) = (I - x_j x^*_j) \\lambda (x_j + t),$$\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$$\n",
    "  (I - x_j x^*_j) (A - \\lambda I) t = (I - x_j x^*_j) (- A x_j + \\lambda x_j) = - (I - x_j x^*_j) A x_j = - (A - R(x_j) I) x_j = -r_j.\n",
    "$$\n",
    "\n",
    "$r_j$ is the residual.\n",
    "\n",
    "Since $(I - x_j x^*_j) t  = t$, we can rewrite this equation in the symmetric form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ (I - x_j x^*_j) (A - \\lambda I) (I - x_j x^*_j) t = -r_j.$$\n",
    "\n",
    "Now we replace $\\lambda$ by $R(x_j)$, and get the **Jacobi correction equation**:\n",
    "\n",
    "$$\n",
    " (I - x_j x^*_j) (A - R(x_j) I) (I - x_j x^*_j) t = -r_j.\n",
    "$$\n",
    "\n",
    "Since $r_j \\perp x_j$ this equation is consistent, if $(A - R(x_j) I)$ is non-singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving Jacobi correction equation\n",
    "\n",
    "Typically Jacobi equation is solved inexactly by the appropriate Krylov method.\n",
    "\n",
    "Even inexact solution of Jacobi equation ensures (why?) that the correction $t$ is orthogonal to $x_j$, which is good for computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection to the Rayleigh quotient iteration\n",
    "\n",
    "If this equation is solved exactly, we will get Rayleigh quotient iteration! Let us show that.\n",
    "\n",
    "$$ (I - x_j x^*_j) (A - R(x_j) I) (I - x_j x^*_j) t = -r_j.$$\n",
    "\n",
    "$$ (I - x_j x^*_j) (A - R(x_j) I) t = -r_j.$$\n",
    "\n",
    "$$  (A - R(x_j) I) t - \\alpha x_j = -r_j, \\quad \\alpha = x^*_j (A - R(x_j) I) x_j$$\n",
    "\n",
    "$$   t = \\alpha (A - R(x_j) I)^{-1}x_j  - (A - R(x_j) I)^{-1}r_j,$$\n",
    "\n",
    "Thus, since $(A - R(x_j) I)^{-1}r_j = (A - R(x_j) I)^{-1}(A - R(x_j) I)x_j = x_j$ we get\n",
    "\n",
    "$$x_{j+1} = x_j + t = \\alpha (A - R(x_j) I)^{-1}x_j,$$\n",
    "\n",
    "which is Rayleigh quotient iteration up to normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preconditioning of Jacobi equation\n",
    "\n",
    "A popular preconditioner for solving Jacobi equation by Krylov method has the form\n",
    "\n",
    "$$\n",
    "\\widetilde K = (I - x_j x^*_j) K (I - x_j x^*_j)\n",
    "$$\n",
    "\n",
    "where $K$ is easily-inverted approximation of $(A - R(x_j) I)$.\n",
    "\n",
    "We need to derive how to solve a system with $\\widetilde K$ in terms of solving a system with $K$.\n",
    "\n",
    "We already showed that equation\n",
    "\n",
    "$$ (I - x_j x^*_j) K (I - x_j x^*_j) \\tilde t = f $$\n",
    "\n",
    "is equavelnt to \n",
    "\n",
    "$$  \\tilde t = \\alpha K^{-1}x_j  + K^{-1}f $$\n",
    "\n",
    "The trick now is to forget about the value of $\\alpha$ and find it from $\\tilde t\\perp x_j$ to maintain orthogonality:\n",
    "\n",
    "$$\n",
    "    \\alpha = \\frac{x_j^*K^{-1}f}{x_j^* K^{-1}x_j}\n",
    "$$\n",
    "Thus for each iteration of the Jacobi equation we calculate $K^{-1}x_j$ and then update only $K^{-1}f$ on each internal Krylov iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Subspace acceleration in JD\n",
    "\n",
    "On each iteration of the method we expand a basis with new $t$.\n",
    "\n",
    "Namely, $V_j = [v_1,\\dots,v_{j-1},v_j]$, where $v_j$ is vector $t$ orthogonalized to $V_{j-1}$.\n",
    "\n",
    "Then standard Rayleigh-Ritz procedure is used.\n",
    "\n",
    "**Historal fact:** Initially subspace acceleration was used in the **Davidson method**. <br> \n",
    "However, instead of the Jacobi equation, equation $(\\mathrm{diag}(A) - R(x_j)I)t = -r_j$ was used. <br>\n",
    "Davidson method was very popular in quantum chemistry computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The block case of JD\n",
    "\n",
    "If we want many eigenvectors, we just compute **partial Schur decomposition:**\n",
    "\n",
    "$$A Q_k = Q_k T_k, $$\n",
    "\n",
    "and then want to update $Q_k$ by one vector added to $Q_k$. We just use instead of $A$ the matrix $(I - Q_k Q^*_k) A (I - Q_k Q^*_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Jacobi-Davidson: summary\n",
    "\n",
    "The correction equation can be solved only roughly, and JD method is often the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Software\n",
    "\n",
    "- The [ARPack](http://www.caam.rice.edu/software/ARPACK/) is the most widely used (it powers scipy sparse eigensolver). Includes versions of Lanczos and Arnoldi algorithms.\n",
    "- The [PRIMME](https://github.com/primme/primme) is the best from my experience (it employs dynamic switching between different methods including LOBPCG and JD)\n",
    "- [PROPACK](http://sun.stanford.edu/~rmunk/PROPACK/) works well for the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take-home message\n",
    "\n",
    "- Arnoldi and Lanczos methods. Shift-and-invert strategy is very expensive since inversion must be done very accurately.\n",
    "- Preconditioned iterative methods (PINVIT, LOBPCG, JD). Good for inexact inversions. \n",
    "- There is a software for using them\n",
    "- There is a lot of technical issues hidden (restarts, spurious eigenvalues, stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "\n",
    "\n",
    "- Fast Fourier transform\n",
    "\n",
    "- Structured matrices (Toeplitz, Circulants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
